{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "input_path = 'dataset/toy_3-core_80_20_with_text.csv'\n",
    "output_path = 'dataset/toy_embdded_review.csv'\n",
    "\n",
    "def load_text(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(1, len(lines)):\n",
    "             record = {}\n",
    "             line = lines[i].split(',')\n",
    "             record['reviewerID'] = line[0]\n",
    "             record['asin'] = line[1]\n",
    "             record['rating'] = line[2]\n",
    "             record['review'] = \"\".join(line[3:])\n",
    "             data.append(record)\n",
    "    return data\n",
    "\n",
    "def get_pure_text(review):\n",
    "    return review['review']\n",
    "\n",
    "def get_review_info(review):\n",
    "    review = review.copy()\n",
    "    review.pop('review')\n",
    "    return review\n",
    "\n",
    "def text2vector(input_path, output_path):\n",
    "    reviews = load_text(input_path)\n",
    "    pure_text = list(map(get_pure_text, reviews))\n",
    "    df_info = pd.DataFrame(list(map(get_review_info, reviews)))\n",
    "    \n",
    "\n",
    "    def tokenize_text(reviews, batch_size):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        reviews: a list of string\n",
    "            contains the text to be converted\n",
    "        batch_size: int\n",
    "            text numbers\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        embedding: tensor\n",
    "            embedded text\n",
    "        \"\"\"\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        current_index = 0\n",
    "        batch_num = len(reviews) // batch_size\n",
    "        embedding = tokenizer(pure_text[current_index:(current_index+batch_size)], return_tensors=\"pt\", padding=\"longest\", truncation=True)['input_ids']\n",
    "        current_index += batch_size\n",
    "        for i in range(1,batch_num):\n",
    "            current_embedding = tokenizer(pure_text[current_index:(current_index+batch_size)], return_tensors=\"pt\", padding=\"longest\", truncation=True)['input_ids']\n",
    "            current_index += batch_size\n",
    "            torch.cat((embedding, current_embedding), dim=0)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "    inputs = tokenize_text(pure_text, 10000)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    emb_dims = model.config.hidden_size\n",
    "    model.to(device)\n",
    "    batch_size = 10 \n",
    "    \n",
    "    outputs = np.zeros((len(pure_text), emb_dims))\n",
    "    torch.cuda.empty_cache()\n",
    "    batch_num = len(inputs) // batch_size + 1\n",
    "    start_time = time.time()\n",
    "    for batch in range(batch_num):\n",
    "        if not batch % 100:\n",
    "            current_time = time.time()\n",
    "            print(f'{batch*batch_size} reviews processed, cost {current_time - start_time}s')\n",
    "        \n",
    "        start_index = batch * batch_size\n",
    "        end_index = min(start_index+batch_size, len(inputs))\n",
    "        selected_inputs = inputs[start_index:end_index]\n",
    "        \n",
    "        # Move input to GPU\n",
    "        selected_inputs = selected_inputs.cuda()\n",
    "        output = model(selected_inputs).last_hidden_state.mean(dim=1)\n",
    "        try:\n",
    "            output = output.cpu().detach().numpy()\n",
    "            outputs[start_index:start_index+batch_size, :] = output\n",
    "        except:\n",
    "            continue\n",
    "        del output\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    df = pd.DataFrame(outputs)\n",
    "    df = pd.concat([df_info, df], axis=1)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 reviews processed, cost 0.0s\n",
      "1000 reviews processed, cost 20.020386457443237s\n",
      "2000 reviews processed, cost 39.91488766670227s\n",
      "3000 reviews processed, cost 59.89401650428772s\n",
      "4000 reviews processed, cost 79.91717219352722s\n",
      "5000 reviews processed, cost 99.93721461296082s\n",
      "6000 reviews processed, cost 120.02234935760498s\n",
      "7000 reviews processed, cost 140.1390676498413s\n",
      "8000 reviews processed, cost 160.25284481048584s\n",
      "9000 reviews processed, cost 180.35614442825317s\n",
      "10000 reviews processed, cost 200.46010303497314s\n",
      "train data processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 reviews processed, cost 0.0s\n",
      "1000 reviews processed, cost 19.943925142288208s\n",
      "2000 reviews processed, cost 39.82104563713074s\n",
      "3000 reviews processed, cost 59.751686334609985s\n",
      "4000 reviews processed, cost 79.74381566047668s\n",
      "5000 reviews processed, cost 99.75318551063538s\n",
      "6000 reviews processed, cost 119.7707028388977s\n",
      "7000 reviews processed, cost 139.83673810958862s\n",
      "8000 reviews processed, cost 159.90662026405334s\n",
      "9000 reviews processed, cost 180.0421531200409s\n",
      "10000 reviews processed, cost 200.10855340957642s\n",
      "val data processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 reviews processed, cost 0.0s\n",
      "1000 reviews processed, cost 20.000468492507935s\n",
      "2000 reviews processed, cost 39.92395997047424s\n",
      "3000 reviews processed, cost 59.91825866699219s\n",
      "4000 reviews processed, cost 79.97499775886536s\n",
      "5000 reviews processed, cost 100.09404921531677s\n",
      "6000 reviews processed, cost 120.1779944896698s\n",
      "7000 reviews processed, cost 140.2665615081787s\n",
      "8000 reviews processed, cost 160.38254499435425s\n",
      "9000 reviews processed, cost 180.52363848686218s\n",
      "10000 reviews processed, cost 200.66764736175537s\n",
      "test data processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k in ['train', 'val', 'test']:\n",
    "    input_path = f'dataset/toy_3-core_80_20_{k}_with_text.csv'\n",
    "    output_path = f'dataset/toy_embedded_review_{k}.csv'\n",
    "    text2vector(input_path, output_path)\n",
    "    print(f'{k} data processed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89e41efaa00d42f1334e801de5c81d94805d2e8968b4879ca6e77e7a8553473c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
